\documentclass {article}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{color}
\usepackage{mathtools}


\title{Bachelor 2015}
\author{Alisa Dammer}

\begin{document}
\maketitle
 
\begin{enumerate}
	\item[1.] Introduction:
		\begin{enumerate}
			\item[1.1] Motivation
			\item[1.2] Overview of the existing approaches, models and programs
			\item[1.3] Brief explanation of the Thesis of the Bachelor
		\end{enumerate}
	\item[2.] The model
		\begin{enumerate}
			\item[2.0] Explanation of the Thesis of the Bachelor
				\begin{enumerate}
					\item[2.0.1] Theoretical background and the problem statement
					\item[2.0.2] The program broken into several steps
						\begin{enumerate}
							\item[2.0.2.1] Data preparation
							\item[2.0.2.2] Model fine-tuning
							\item[2.0.2.3] Checking the hypothesis for specified limitations
						\end{enumerate}
				\end{enumerate} 
			\item[2.1] Data preparation (correlation matrices and further filtering)
			\item[2.2] Model specification and limitations
				\begin{enumerate}
					\item[2.2.1] Specification of the quality and quantity of the parameters
					\item[2.2.2] Iterative fine-tuning of the model 
					\item[2.2.3] Error check
				\end{enumerate}
			\item[2.3] Second data processing (sentimental analysis for the found indexes)
			\item[2.4] Integration of the sub-results into final result
			\item[2.5] Testing of the result
		\end{enumerate}
	\item[3.] Conclusion
		\begin{enumerate}
			\item[3.1] The interpretation of the results
			\item[3.2] Bottle neck and other problems of the model
			\item[3.3] The potential and further use
			\item[3.4] Possible improvements
		\end{enumerate}
	\item[4.] References
\end{enumerate}

\newpage
\section{Introduction}
\subsection{Motivation}
Big Data\\
Data mining\\
Sentiment data analysis
\subsection{An overview of the existing approaches, models and programs}
classical econometric - regressions\\
Among the classical econometric approaches two main directions can be called: linear and non-linear regressions. First of all we will consider linear regressions and discuss their advantages and disadvantages.\\
According to the most common definition, in statistics, linear regression is an approach for modeling the relationship between a scalar dependent variable y and explanatory variable denoted X [1]. There are one-variable regressions called simple linear regressions and multiple regressions, which shows the dependencies of the dependent variable from more than one explanatory variables. The regression equation looks following way:\\
\[Y = a + bX + \varepsilon\]
where Y - dependent variable, X - explanatory variable, a - the intercept of regression line, b - the slope of the regression line, $\varepsilon$ - the error term.\\
The simple linear regression is based on 6 strict assumptions:
\begin{enumerate}
	\item Because of the linear form neither parameter a, nor parameter b may have higher power than one.
	\item The independent variable X is not random.
	\item The variance is constant. Which means that the expected error value is 0.
	\item The variance of $\varepsilon$ is constant for all observations.
	\item Errors are not correlated.
	\item The error's distribution is normal.
\end{enumerate}
On one hand the simple linear regression has several disadvantages: the form of the equation (here number of the predicting variables is concerned) and underlying assumptions. The dependency from only one variable can not describe fully all the relationship on the market. Even if you consider an equations system, consisting from linear regressions with different variables, it will lead to unnecessary complexity and not realistic or complete dependencies between variables. The second disadvantage is the assumptions - they are unrealistic in modern world and are appropriate only for very simplified model of he world.\\
The big advantage of the simple linear regression on the other hand is it's estimators: ordinary lest squares, generalized least squares, instrumental variables, maximum likelihood (ML) and various ML techniques - these are the most known and not too complex approaches to estimate the regression. There also many other different techniques, but we won't discuss them here. An example of a simple linear regression could be dependency between the harvest volume and amount of rain during the season. Let's say Y = amount of harvest (potato for example) in kg and X is amount of the precipitation in mm\\ Than the more rain will drop during the season the bigger amount of potato will be harvested at the end. The slope of the regression line is positive. And the intercept is also more than 0, because you can't dig out less, than you have planted. So, the regression will look like:
\[amount \thinspace of \thinspace harvest = |a| + |b|*amount \thinspace of \thinspace rain \epsilon\] \\
Since it is likely to be impossible to build a proper prediction for the variable only with one predicting variable, multiple linear regression was invented. As you can see from the name, multiple linear regression describes relationship between more than one explanatory variables and dependent variable. The form of the equation looks as following:\[ Y = a + b_{1}X_{1}+ ... + b_{n}X_{n} + \epsilon, \thinspace \thinspace where \thinspace \thinspace n \in N\]   
$b_{i}$ - the "contribution" of the i-th variable to the regression, a -the intercept of the regression line, Y - dependant variable, X - explanatory variables.\\
Unlike simple linear regression, multiple linear regression does not have the form-disadvantage. However, the linear nature of the regression provides the model with several assumptions:
\begin{enumerate}
	\item All $b_{i}$ have the power of 1 - linear dependency between Y and all Xi-th.
	\item The residuals are distributed normally. (Residuals - the difference between predicted value and observed value).
	\item Uncertain number of the explanatory variables X - there is no technique to choose exact number of the variables that will optimally predict the value Y.
	\item Completely "substitutive" variables X - unnecessary big number of the variables without increasing the accuracy of the prediction.
	\item Complex form of the variables (X can have a higher-order polynomial form) leads to reduction of transparency and wrong results. 
\end{enumerate}
The advantage of the multiple linear regression is that it can be tested several ways. First, you can test whether the regression has the linear character by using F-test or ANOVA table. Second, to check whether the model is good enough you can use the same tests as for simple linear regression: determination coefficient $R^2$. You can also run several hypothesis-test for each explanatory variable ($b_{i}$ coefficient is equal to 0 - the variable is insignificant).\\
As an example of the multiple linear regression we can take previous example and extend it. The amount of the precipitation is not the only factor for the harvest to grow. Our Y variable also depends on amount of sunny days ($X_{2}$) and average temperature during the season ($X_{3}$). The higher temperature, the better will be the harvest (in reality this is a non linear dependency; most likely the dependency here takes the form of $(c-d*\sqrt{x})$: the harvest will be big, if the temperature is between certain degrees). The higher number of the sunny days the less rainy days, but higher the temperature. This is an example where two explanatory variables are not complete substitutes, but they have negative dependency with each other, so, their dependency will be explicitly included into the regression. The general form of the equation will take following form:
\[harvest = |a| + |b_{1}|*rain + |b_{2}|*sun + |b_{3}|*temperature + |b_{12}|*sun*rain + \epsilon\]
So the multiple linear regression is better for real purposes (real market analysis) in comparison to simple linear regression, but it still contains restrictions that do not allow high accuracy prediction.\\
Unfortunately the linear character of the dependencies is a rare thing in real life. That is why nonlinear regressions took place in statistical analysis. The general form of the nonlinear regression looks as following:
\[Y = f(\beta, x'_{i}) + \epsilon'\]
where f() - nonlinear function, $x'_{i}$ - vector of predictors, $\epsilon$ - vector of parameters, $\epsilon_{i} \sim N(0, \sigma^2)$.\\
Nonlinear regressions can be divided into two main classes:
\begin{enumerate}
	\item Nonlinear dependencies between explanatory variables ($x_{i}$) and dependent variable (Y), linear character of the parameters ($\beta$).
	\item Linear dependencies between explanatory variables ($x_{i}$) and dependent variable (Y), nonlinear character or the parameters ($\beta$) 
\end{enumerate} 
The first class is relatively easy to work with: you can use variables substitution and bring the original equation to linear form and than use same techniques as for linear multiple regression, for example, you can use least squares method. Additional to LS you will need to change the form of the estimates, because they were found not for original form of the variables f(x), but for $f^{-1}(x)$. This x = $f^{-1}(x)$ is a substitute to get the linear form for the regression [non linear j]. As the last step you will need to find the prediction for the original dependent variable and not for it's transformed form.\\
The second class is very difficult to deal with. Because of the nonlinear nature of the parameters you can not use normal estimation techniques. One one hand you can use the ordinary optimum finding algorithms, but you will run into different difficulties because of the character of the dependencies. For those cases there are exist different approaches: Gauss-Newton method [nonlinear m],  Levenberg-Marquardt method [nonlinear n]. You can also iterative try to use LS method, but with each iteration the estimates will get additional error, which can result in immense accuracy decrease. \\
As an example for a nonlinear regression we can take the earlier described dependency between harvest volume, amount of rain, number of sunny days and temperature.
\[harvest = a + b_{1}*ln(sun) + b_{2}*(c-d*(rain-e)^2) +b_{3}*f(temperature) + b_{12}*g(sun, rain)\]
where $f(n) = \left\{ 
  \begin{array}{l l}
  	0,\thinspace x < 10 \\
    \alpha x, \thinspace x \in [10, 30] and \alpha > 1 \\
    -\alpha x,\thinspace x > 30
  \end{array} \right.$ ; c, d and e $>$ 0, $b_{i}$ -reggresion coefficients here they are first-order parameters. \\
As you can see,the normal approach here is is almost impossible to use, moreover, here we see different form of dependencies at one time (normally people use uniform functions like polynomials, logarithms etc.) But this equation can be transformed into linear form via several steps of the variables substitution and transformations, then the LS will be found and the process of "backwards transformation" will be started, until we get the estimates for the original variables.

neural networks\\

machine learning\\
Most successful approaches: Top 5
\subsection{Explanation of the Thesis of the Bachelor}
The idea: "how to check the hypothesis" - in several logical steps.

\newpage
\section{The model}
\subsection{Data preparation}
Here independent of the main index (not chosen yet) based on correlation-matrices certain amount of indexes will be taken for further consideration. For example every index with correlation-vector bigger or equal to +-0.3\\
Some period of the time-series (for all indexes) will be left as test-sample. (let's say about a month-period).\\
All time series will be cleaned from the noise using following technics: mooving average, extracting a trend line on high frequency long-term data, 
\subsection{Model specification and limitations}
First, the equation or the model description will be given, then the number for the parameters will be set (with all necessary explanations).\\
Second, the automatic optimum finding for given limitations will be maid. (python iterative optimum-finding test). At the end another target function, built from the found optimum will be tested (error check).\\
Third, texts for the chosen indexes will be analysed. The forecast, probability and maybe buzz will be the results of the sub-chapter.\\
Forth, the results of the previous stage will be united into one forecast and probability for the main index (index on the left side of the main equation).\\
Finally, the results of the model will be compared with the actual results that we left as the test-sample.

\newpage
\section{conclusion}
\subsection{Interpretation of the results}
Here the difference of the actual value (test-sample) and our forecasts will be discussed and explained.
\subsection{Problems}
Here the biggest difficulties will be stated and also discussed the influence of the strong limitations.
\subsection{Potential}
Here most possible use and advantage of the model will be discussed.
\subsection{Improvements}
In order to be more useful the program can take several improvements...\\
Theoretical\\
Data\\
Technical

\newpage
\section{References}
\begin{enumerate}
	\item Wikipedia: linear regeression \url{http://en.wikipedia.org/wiki/Linear_regression}
\end{enumerate}

\end{document}